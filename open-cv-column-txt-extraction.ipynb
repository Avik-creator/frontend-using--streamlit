{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YM1yRwQdRPhG"
   },
   "source": [
    "# **Column Wise Approach**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Installation Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:51:31.662875Z",
     "iopub.status.busy": "2024-12-15T17:51:31.662054Z",
     "iopub.status.idle": "2024-12-15T17:51:31.750679Z",
     "shell.execute_reply": "2024-12-15T17:51:31.749670Z",
     "shell.execute_reply.started": "2024-12-15T17:51:31.662843Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File in pdf format\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import fitz\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "resume_link = '/kaggle/input/resume-dataset/Resumes/DUAL_COLUMN_ENGLISH_DATASETS/Resume129.pdf'\n",
    "if resume_link.endswith('.pdf'):\n",
    "    print(\"File in pdf format\")\n",
    "\n",
    "    doc = fitz.open(resume_link)\n",
    "#     print(len(doc))\n",
    "\n",
    "    # Loop through each page in the PDF\n",
    "    for page_number in range(len(doc)):\n",
    "       \n",
    "        page = doc.load_page(page_number)  # Get the page\n",
    "        pix = page.get_pixmap()  # Convert the page to an image\n",
    "        if page_number == 0:\n",
    "            output_image_path = \"/kaggle/working/original_image.jpg\"\n",
    "            # Save the image as a JPG file\n",
    "            pix.save(output_image_path)\n",
    "        else:\n",
    "            pix.save(f\"/kaggle/working/original_image(Page-{page_number}).jpg\")\n",
    "            print(f\"Re-run all below code by changing in image path of YOLO by /kaggle/working/original_image_(Page-{page_number}).jpg\")\n",
    "#     if len(doc) > 1:\n",
    "#         print(\"Still only one page resume is taken into account\")\n",
    "    # Close the document\n",
    "    doc.close()\n",
    "elif resume_link.endswith('.jpg') or resume_link.endswith('jpeg') or resume_link.endswith('png'):\n",
    "    print(\"File in image format\")\n",
    "    img = cv2.imread(resume_link)\n",
    "    cv2.imwrite('/kaggle/working/original_image.jpg', img)\n",
    "else:\n",
    "    print(\"File format not supported!! [Supported formats: .jpg/.png/.jpeg/.pdf]\")\n",
    "# else: ######have to work on this\n",
    "#         for page_number in range(len(doc)):\n",
    "#             page = doc.load_page(page_number)  # Get the page\n",
    "#             pix = page.get_pixmap()  # Convert the page to an image\n",
    "\n",
    "#             # Open the images you want to combine\n",
    "#             o1 = \"/kaggle/working/original_image.jpg\"\n",
    "#             if os.path.exists(o1):\n",
    "#                 print(0)\n",
    "#                 image1_arr = cv2.imread(o1)  # Image 1 as NumPy array\n",
    "\n",
    "#                 # Convert pixmap to PIL image\n",
    "#                 image2_pil = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "#                 image2 = cv2.cvtColor(np.array(image2_pil), cv2.COLOR_RGB2BGR)  # Convert to OpenCV format\n",
    "\n",
    "#                 # Get dimensions for image1 and image2 using shape (height, width, channels)\n",
    "#                 h1, w1, _ = image1_arr.shape\n",
    "#                 h2, w2, _ = image2.shape\n",
    "\n",
    "#                 # Create a new blank image for vertical combination\n",
    "#                 new_image_v = np.zeros((h1 + h2, max(w1, w2), 3), dtype=np.uint8)\n",
    "                \n",
    "#                 image1 = cv2.cvtColor(np.array(image1_arr), cv2.COLOR_BGR2RGB)\n",
    "#                 # Paste image1 and image2 onto new_image_v\n",
    "#                 new_image_v[:h1, :w1] = image1  # Place image1\n",
    "#                 new_image_v[h1:h1+h2, :w2] = image2  # Place image2 below image1\n",
    "\n",
    "#                 # Save the combined image\n",
    "#                 new_image = Image.fromarray(new_image_v)\n",
    "#                 new_image.save(\"/kaggle/working/original_image.jpg\")\n",
    "\n",
    "#             else:\n",
    "#                 print(1)\n",
    "#                 pix.save(o1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Removal of photo**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:51:32.809105Z",
     "iopub.status.busy": "2024-12-15T17:51:32.808499Z",
     "iopub.status.idle": "2024-12-15T17:51:40.111190Z",
     "shell.execute_reply": "2024-12-15T17:51:40.110282Z",
     "shell.execute_reply.started": "2024-12-15T17:51:32.809071Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=/kaggle/working/resized_image.jpg, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.01, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=/kaggle/working/, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 ðŸš€ v7.0-389-ge62a31b6 Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "image 1/1 /kaggle/working/resized_image.jpg: 640x640 1 person, 1 laptop, 2 books, 12.2ms\n",
      "Speed: 0.6ms pre-process, 12.2ms inference, 174.5ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1m/kaggle/working/exp\u001b[0m\n",
      "1 labels saved to /kaggle/working/exp/labels\n"
     ]
    }
   ],
   "source": [
    "# Perform object detection using YOLOv5\n",
    "import cv2\n",
    "image =  \"/kaggle/working/original_image.jpg\" #If needed change the link for multiple pages\n",
    "# %store image\n",
    "im1 = cv2.imread(image)\n",
    "fixed_size = (640,640)\n",
    "r_image = cv2.resize(im1, fixed_size)\n",
    "cv2.imwrite('/kaggle/working/resized_image.jpg', r_image)\n",
    "#Save results in a custom directory\n",
    "output =  \"/kaggle/working/\"\n",
    "# Perform object detection using YOLOv5\n",
    "!python detect.py --source '/kaggle/working/resized_image.jpg' --weights yolov5s.pt --img 640 --conf 0.01 --project \"{output}\" --save-txt --name exp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:51:40.113375Z",
     "iopub.status.busy": "2024-12-15T17:51:40.113082Z",
     "iopub.status.idle": "2024-12-15T17:51:40.591694Z",
     "shell.execute_reply": "2024-12-15T17:51:40.590951Z",
     "shell.execute_reply.started": "2024-12-15T17:51:40.113347Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
      "YOLOv5 ðŸš€ 2024-12-15 Python-3.10.14 torch-2.4.0 CUDA:0 (Tesla T4, 15095MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
      "Adding AutoShape... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coordinates: x1=62, y1=36, x2=198, y2=146\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the YOLOv5 model (YOLOv5s for small model)\n",
    "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
    "\n",
    "# Load the image\n",
    "im1 = cv2.imread(\"/kaggle/working/exp/resized_image.jpg\")####################\n",
    "\n",
    "image = cv2.imread(\"/kaggle/working/resized_image.jpg\")#####################\n",
    "\n",
    "# Read from YOLOv5 output text file\n",
    "image_width = 640\n",
    "image_height = 640\n",
    "output_file = '/kaggle/working/exp/labels/resized_image.txt'  ################ Path to the saved results\n",
    "x1=x2=y1=y2 = 0.000000\n",
    "c = 0\n",
    "if os.path.isfile(output_file):\n",
    "    with open(output_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            values = line.split()\n",
    "            class_id = int(values[0])\n",
    "            if class_id == 0:\n",
    "                if c == 1:\n",
    "                    height_norm1 = float(values[4])\n",
    "                    if height_norm1 > height_norm:\n",
    "                        break\n",
    "                c = c + 1\n",
    "                x_center_norm = float(values[1])\n",
    "                y_center_norm = float(values[2])\n",
    "                width_norm = float(values[3])\n",
    "                height_norm = float(values[4])\n",
    "\n",
    "                # Convert to pixel coordinates\n",
    "                x_center = x_center_norm * image_width\n",
    "                y_center = y_center_norm * image_height\n",
    "                width = width_norm * image_width\n",
    "                height = height_norm * image_height\n",
    "\n",
    "                x1 = x_center - (width / 2)\n",
    "                y1 = y_center - (height / 2)\n",
    "                x2 = x_center + (width / 2)\n",
    "                y2 = y_center + (height / 2)\n",
    "\n",
    "    x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "    print(f\"Coordinates: x1={x1}, y1={y1}, x2={x2}, y2={y2}\")\n",
    "            \n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (255, 255, 255), -1)\n",
    "\n",
    "# Save or display the result\n",
    "cv2.imwrite('/kaggle/working/results_person_removed.jpg', image)\n",
    "#-----------------------------------------------------------------------------------------------------        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:51:40.593022Z",
     "iopub.status.busy": "2024-12-15T17:51:40.592765Z",
     "iopub.status.idle": "2024-12-15T17:51:41.604084Z",
     "shell.execute_reply": "2024-12-15T17:51:41.602816Z",
     "shell.execute_reply.started": "2024-12-15T17:51:40.592997Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/exp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Checking if the image is already greyscaled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:51:41.606914Z",
     "iopub.status.busy": "2024-12-15T17:51:41.606566Z",
     "iopub.status.idle": "2024-12-15T17:51:41.624957Z",
     "shell.execute_reply": "2024-12-15T17:51:41.624091Z",
     "shell.execute_reply.started": "2024-12-15T17:51:41.606883Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is not grayscale.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def is_grayscale(image_path):\n",
    "    # Load the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Check if the image is single channel (grayscale)\n",
    "    if len(image.shape) == 2:  # Grayscale image will have shape (height, width)\n",
    "        return True\n",
    "    \n",
    "    # If the image has 3 channels, check if all channels are the same\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:  # RGB image will have shape (height, width, 3)\n",
    "        # Split the image into its three channels\n",
    "        b, g, r = cv2.split(image)\n",
    "        # Check if all channels are equal (grayscale has equal R, G, B values)\n",
    "        if np.array_equal(b, g) and np.array_equal(g, r):\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Example usage\n",
    "image_path = '/kaggle/working/results_person_removed.jpg'\n",
    "if is_grayscale(image_path):\n",
    "    print(\"Image is grayscale\")\n",
    "    grayscale_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    # Increase the brightness and contrast of the image (if needed)\n",
    "    alpha = 1  # Contrast control (1.0-3.0)\n",
    "    beta = 0    # Brightness control (0-100)\n",
    "\n",
    "    # Adjust brightness and contrast\n",
    "    brightened_image = cv2.convertScaleAbs(grayscale_image, alpha=alpha, beta=beta)\n",
    "\n",
    "    # Apply a threshold to turn gray background to white\n",
    "    _, thresholded_image = cv2.threshold(brightened_image, 200, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "    # Convert the resulting image to RGB\n",
    "    rgb_image = cv2.cvtColor(thresholded_image, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Save or display the RGB image\n",
    "    cv2.imwrite('/kaggle/working/results_image.jpg', rgb_image)\n",
    "else:\n",
    "    print(\"The image is not grayscale.\")\n",
    "    rgb_image = cv2.imread(image_path)\n",
    "    cv2.imwrite('/kaggle/working/results_image.jpg', rgb_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Creating 2 columns basis of space**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:51:41.626133Z",
     "iopub.status.busy": "2024-12-15T17:51:41.625897Z",
     "iopub.status.idle": "2024-12-15T17:51:41.630537Z",
     "shell.execute_reply": "2024-12-15T17:51:41.629766Z",
     "shell.execute_reply.started": "2024-12-15T17:51:41.626110Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "\n",
    "# # Assuming you've already loaded the image\n",
    "# image = cv2.imread('/kaggle/working/results_image.jpg')\n",
    "\n",
    "# def count_distinct_colors(image, color_space='RGB'):\n",
    "#     # Convert image to the specified color space\n",
    "#     if color_space == 'HSV':\n",
    "#         converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "#     elif color_space == 'Lab':\n",
    "#         converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "#     else:\n",
    "#         converted_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "#     # Reshape the image to a 2D array of pixels\n",
    "#     pixels = converted_image.reshape(-1, 3)\n",
    "    \n",
    "#     # Convert to a set of tuples (each tuple represents a unique color)\n",
    "#     unique_colors = set(map(tuple, pixels))\n",
    "    \n",
    "#     return len(unique_colors)\n",
    "\n",
    "# # Count distinct colors in different color spaces\n",
    "# rgb_count = count_distinct_colors(image, 'RGB')\n",
    "# hsv_count = count_distinct_colors(image, 'HSV')\n",
    "# lab_count = count_distinct_colors(image, 'Lab')\n",
    "\n",
    "# print(f\"Number of distinct colors in RGB space: {rgb_count}\")\n",
    "# print(f\"Number of distinct colors in HSV space: {hsv_count}\")\n",
    "# print(f\"Number of distinct colors in Lab space: {lab_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:51:41.632125Z",
     "iopub.status.busy": "2024-12-15T17:51:41.631796Z",
     "iopub.status.idle": "2024-12-15T17:51:41.646143Z",
     "shell.execute_reply": "2024-12-15T17:51:41.645331Z",
     "shell.execute_reply.started": "2024-12-15T17:51:41.632090Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1725285496764,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "QP5EQing0t7R",
    "outputId": "930ac406-1130-46cd-d312-ccd9452920a0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the image\n",
    "# r_image = cv2.imread(\"/kaggle/working/results_person_removed.jpg\")\n",
    "# fixed_size = (1250,1563)\n",
    "# image = cv2.resize(r_image, fixed_size)\n",
    "# cv2.imwrite(\"/kaggle/working/2nd_resized_image.jpg\",image)\n",
    "# image = cv2.imread(\"/kaggle/working/2nd_resized_image.jpg\")\n",
    "# # Convert the image to grayscale\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # Apply binary thresholding to get a binary image\n",
    "# _, binary = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# # Find the contours of the text regions\n",
    "# contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# # Create a mask for the text regions\n",
    "# mask = np.zeros_like(binary)\n",
    "\n",
    "# # Draw filled contours on the mask (text regions become white)\n",
    "# cv2.drawContours(mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "# # Invert the mask to get the non-text areas (non-text areas become white)\n",
    "# mask_inv = cv2.bitwise_not(mask)\n",
    "# # Find the contours of the text regions\n",
    "# cv2.drawContours(mask_inv, contours, 1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "# unique_background_colors = 1\n",
    "\n",
    "# for contour in contours:\n",
    "# #     print(contour)\n",
    "#     x, y, w, h = cv2.boundingRect(contour)\n",
    "# #     cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "\n",
    "#     if h > 500 and w > 150:\n",
    "#         unique_background_colors = 2\n",
    "       \n",
    "# if unique_background_colors == 2:  # Two background colors detected\n",
    "#     print(\"Two background colors detected. Keeping the image unchanged.\")\n",
    "# else:\n",
    "# # Parameters\n",
    "#     height, width = mask_inv.shape\n",
    "#     line_thickness = 2\n",
    "#     required_non_text_percentage = 0.98\n",
    "#     # 98% non-text space requirement(0.9780 - 0.98)\n",
    "#     required_consecutive_columns =  40\n",
    "#     #Number of consecutive columns that need to meet the non-text requirement(40-25)\n",
    "#     non_text_threshold = int(height * required_non_text_percentage)\n",
    "#     initial_skip_percentage = 0.2      # Skip the first 20% of the x-axis#@@@@@@@@@@@@@@@@@\n",
    "\n",
    "#     # Calculate the starting point after skipping the initial part\n",
    "#     start_x = int(width * initial_skip_percentage)\n",
    "\n",
    "#     # Initialize flag and counters\n",
    "#     consecutive_non_text_columns = 0\n",
    "#     best_x = -1\n",
    "\n",
    "#     # Scan across the x-axis starting after the initial skip\n",
    "#     for x in range(start_x, width):\n",
    "#         # Calculate the number of non-text pixels along the y-axis at this x position\n",
    "#         non_text_pixels = np.sum(mask_inv[:, x]) / 255\n",
    "\n",
    "#         # Check if this position meets the non-text requirement\n",
    "#         if non_text_pixels >= non_text_threshold:\n",
    "#             consecutive_non_text_columns += 1\n",
    "#             # If we have enough consecutive columns, we can consider this position\n",
    "#             if consecutive_non_text_columns >= required_consecutive_columns:\n",
    "#                 best_x = x - (consecutive_non_text_columns // 2)  # Center the line\n",
    "#                 break\n",
    "#         else:\n",
    "#             # Reset the counter if the requirement is not met\n",
    "#             consecutive_non_text_columns = 0\n",
    "\n",
    "#     # Draw the vertical line at the found position if it exists\n",
    "#     if best_x != -1:\n",
    "#          # Define the background color (e.g., light blue)\n",
    "#         background_color = [255, 100, 255]  # BGR format for light blue\n",
    "\n",
    "#         # Create a colored rectangle for the background change\n",
    "#         background = np.full_like(image, background_color)# BGR format for light blue\n",
    "\n",
    "#          # Convert the mask to a three-channel image to match the color image\n",
    "#         mask_3channel = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "#         # Apply the background color only to the area before the line, ignoring text\n",
    "#         image[:, :best_x] = np.where(mask_3channel[:, :best_x] == [0, 0, 0], background[:, :best_x], image[:, :best_x])\n",
    "\n",
    "#         cv2.line(image, (best_x, 0), (best_x, height), (0,0,0), thickness=line_thickness)\n",
    "#         print(f\"Line drawn at x={best_x}.\")\n",
    "#     else:\n",
    "#         print(\"No suitable vertical space found for drawing the line.\")\n",
    "\n",
    "# # Save or display the result\n",
    "# cv2.imwrite('/kaggle/working/output_image_with_line.png', image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:51:41.647422Z",
     "iopub.status.busy": "2024-12-15T17:51:41.647139Z",
     "iopub.status.idle": "2024-12-15T17:51:49.302170Z",
     "shell.execute_reply": "2024-12-15T17:51:49.301371Z",
     "shell.execute_reply.started": "2024-12-15T17:51:41.647398Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "[array([64, 64, 64], dtype=uint8), array([120, 120, 120], dtype=uint8), array([176, 176, 176], dtype=uint8), array([232, 232, 232], dtype=uint8)]\n",
      "No. of background parts with different colour found:  1\n",
      "Line drawn at x=450 with 90 consecutive columns with 100.0% of non-text\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "# Load the image\n",
    "r_image = cv2.imread(\"/kaggle/working/results_image.jpg\")\n",
    "fixed_size = (1250,1563)\n",
    "image = cv2.resize(r_image, fixed_size)\n",
    "cv2.imwrite(\"/kaggle/working/2nd_resized_image.jpg\",image)\n",
    "image = cv2.imread(\"/kaggle/working/2nd_resized_image.jpg\")\n",
    "\n",
    "\n",
    "# Convert the image to grayscale\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "a = 0\n",
    "for i in range(70,150,5):\n",
    "    # Apply binary thresholding to get a binary image\n",
    "    _, binary = cv2.threshold(gray, i, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Find the contours of the text regions\n",
    "    contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    # Create a mask for the text regions\n",
    "    mask = np.zeros_like(binary)\n",
    "\n",
    "    # Draw filled contours on the mask (text regions become white)\n",
    "    cv2.drawContours(mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    # Invert the mask to get the non-text areas (non-text areas become white)\n",
    "    mask_inv = cv2.bitwise_not(mask)\n",
    "    # Get the unique pixel values\n",
    "    # unique_colors = np.unique(gray)\n",
    "    # # print(unique_colors)\n",
    "    # prev_unique_color = unique_colors_count = 0\n",
    "    # for i in unique_colors:\n",
    "    #     if i - prev_unique_color > 55:\n",
    "    #         prev_unique_color = i\n",
    "    #         print(i)\n",
    "    #         unique_colors_count = unique_colors_count+1\n",
    "    # print(unique_colors_count)\n",
    "    #Find the contours of the text regions\n",
    "    # Convert the image to HSV color space\n",
    "    converted_image = cv2.cvtColor(r_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Reshape the image to a 2D array of pixels\n",
    "    pixels = converted_image.reshape(-1, 3)\n",
    "\n",
    "    # Convert to a set of unique colors\n",
    "    unique_colors = set(map(tuple, pixels))\n",
    "\n",
    "    # Convert the set back to an array and sort it by the first channel (Hue)\n",
    "    unique_colors_array = np.array(list(unique_colors))\n",
    "    unique_colors_array = unique_colors_array[unique_colors_array[:, 0].argsort()]\n",
    "\n",
    "    # Initialize the colors list with the first unique color's first channel value\n",
    "    colors = [unique_colors_array[0]]\n",
    "\n",
    "    # Iterate through the unique colors (sorted)\n",
    "    for i in range(1, len(unique_colors_array)):\n",
    "        # Only add the color if the difference with the last color is greater than 55\n",
    "        if abs(unique_colors_array[i][0] - colors[-1][0]) > 55:\n",
    "            colors.append(unique_colors_array[i])\n",
    "\n",
    "    # Print the selected colors\n",
    "    print(colors)\n",
    "    if len(colors)> 2:\n",
    "        unique_background_colors = 1\n",
    "        # Draw filled contours on the mask (text regions become white)\n",
    "    #     cv2.drawContours(mask_inv, contours, 1, 255, thickness=cv2.FILLED)\n",
    "    else:\n",
    "        unique_background_colors = 0 \n",
    "    cv2.drawContours(mask_inv, contours, 0, 255, thickness=cv2.FILLED)\n",
    "\n",
    "# unique_background_colors = 0 # 0/1\n",
    "# cv2.drawContours(mask_inv, contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "    prev_contour = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        if h > 1000 and w > 80 and w < 1200 :#h =800\n",
    "#             print(h)\n",
    "            a = 1\n",
    "#             cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 3)\n",
    "            unique_background_colors = unique_background_colors + 1\n",
    "        \n",
    "    if a==1:\n",
    "        break\n",
    "print(\"No. of background parts with different colour found: \",unique_background_colors)\n",
    "\n",
    "     \n",
    "if unique_background_colors > 1:  # Two background colors detected\n",
    "    print(\"Two background colors detected. Keeping the image unchanged.\")\n",
    "else:\n",
    "\n",
    "# Parameters\n",
    "    height, width = mask_inv.shape\n",
    "    line_thickness = 2\n",
    "    required_non_text_percentage = 0.1\n",
    "    # 98% non-text space requirement(0.9780 - 0.98)\n",
    "    required_consecutive_columns =  40\n",
    "    #Number of consecutive columns that need to meet the non-text requirement(40-25)\n",
    "   \n",
    "    initial_skip_percentage = 0.2      # Skip the first 20% of the x-axis#@@@@@@@@@@@@@@@@@\n",
    "    f = 0\n",
    "    c = 0 \n",
    "    best_x = -1\n",
    "    for required_consecutive_columns in range(65,25,-1):\n",
    "        y = required_consecutive_columns\n",
    "        for required_non_text_percentage1 in range (1000,949,-1):####1000000- to be precise\n",
    "            required_non_text_percentage = required_non_text_percentage1 / 1000.00\n",
    "            x = required_non_text_percentage\n",
    "            if required_non_text_percentage > 0.99 and c==0:\n",
    "                c = 1\n",
    "                for required_consecutive_columns in range(200,10,-1):\n",
    "                        non_text_threshold = int(height * required_non_text_percentage)\n",
    "                        # Calculate the starting point after skipping the initial part\n",
    "                        start_x = int(width * initial_skip_percentage)\n",
    "\n",
    "                        # Initialize flag and counters\n",
    "                        consecutive_non_text_columns = 0\n",
    "\n",
    "                        # Scan across the x-axis starting after the initial skip\n",
    "                        for x in range(start_x, width):\n",
    "                            # Calculate the number of non-text pixels along the y-axis at this x position\n",
    "                            non_text_pixels = np.sum(mask_inv[:, x]) / 255\n",
    "\n",
    "                            # Check if this position meets the non-text requirement\n",
    "                            if non_text_pixels >= non_text_threshold:\n",
    "                                consecutive_non_text_columns += 1\n",
    "                                # If we have enough consecutive columns, we can consider this position\n",
    "                                if consecutive_non_text_columns >= required_consecutive_columns:\n",
    "                                    best_x = x - (consecutive_non_text_columns // 2)  \n",
    "                                    break\n",
    "                            else:\n",
    "                                # Reset the counter if the requirement is not met\n",
    "            #                     print(x)\n",
    "                                consecutive_non_text_columns = 0\n",
    "                        if best_x < 900 and best_x!= -1:\n",
    "                            f = 1\n",
    "                            break\n",
    "        \n",
    "#             if f == 1:\n",
    "#                 # Define the background color (e.g., light blue)\n",
    "#                 background_color = [255, 100, 255]  # BGR format for light blue\n",
    "\n",
    "#                 # Create a colored rectangle for the background change\n",
    "#                 background = np.full_like(image, background_color)# BGR format for light blue\n",
    "\n",
    "#                 # Convert the mask to a three-channel image to match the color image\n",
    "#                 mask_3channel = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "#                 # Apply the background color only to the area before the line, ignoring text\n",
    "#                 image[:, :best_x] = np.where(mask_3channel[:, :best_x] == [0, 0, 0], background[:, :best_x], image[:, :best_x])\n",
    "#                 cv2.line(image, (best_x, 0), (best_x, height), (0,0,0), thickness=line_thickness)\n",
    "#                 required_non_text_percentage = required_non_text_percentage*100\n",
    "#                 print(f\"Line drawn at x={best_x} with {required_consecutive_columns} consecutive columns with {required_non_text_percentage}% of non-text\")\n",
    "#                 break\n",
    "#                         if best_x == -1 or f==0:\n",
    "#                             print(\"No suitable vertical space found for drawing the line.\")       \n",
    "                        \n",
    "            else: \n",
    "                required_non_text_percentage = x\n",
    "                required_consecutive_columns = y\n",
    "                non_text_threshold = int(height * required_non_text_percentage)\n",
    "                # Calculate the starting point after skipping the initial part\n",
    "                start_x = int(width * initial_skip_percentage)\n",
    "                # Initialize flag and counters\n",
    "                consecutive_non_text_columns = 0\n",
    "\n",
    "                # Scan across the x-axis starting after the initial skip\n",
    "                for x in range(start_x, width):\n",
    "                    # Calculate the number of non-text pixels along the y-axis at this x position\n",
    "                    non_text_pixels = np.sum(mask_inv[:, x]) / 255\n",
    "\n",
    "                    # Check if this position meets the non-text requirement\n",
    "                    if non_text_pixels >= non_text_threshold:\n",
    "                        \n",
    "                        consecutive_non_text_columns += 1\n",
    "                        # If we have enough consecutive columns, we can consider this position\n",
    "                        if consecutive_non_text_columns >= required_consecutive_columns:\n",
    "#                             print(non_text_pixels)\n",
    "                            best_x = x - (consecutive_non_text_columns // 2)\n",
    "                            break\n",
    "                    else:\n",
    "                        # Reset the counter if the requirement is not met\n",
    "#                         print(x)\n",
    "                        consecutive_non_text_columns = 0\n",
    "                        \n",
    "                    if best_x < 1000 and best_x != -1:\n",
    "#                         print(best_x)\n",
    "                        f = 1\n",
    "                        break\n",
    "        \n",
    "                \n",
    "            if f == 1:\n",
    "                # Define the background color (e.g., light blue)\n",
    "                background_color = [255, 100, 255]  # BGR format for light blue\n",
    "\n",
    "                # Create a colored rectangle for the background change\n",
    "                background = np.full_like(image, background_color)# BGR format for light blue\n",
    "\n",
    "                # Convert the mask to a three-channel image to match the color image\n",
    "                mask_3channel = cv2.cvtColor(mask, cv2.COLOR_GRAY2BGR)\n",
    "                # Apply the background color only to the area before the line, ignoring text\n",
    "                image[:, :best_x] = np.where(mask_3channel[:, :best_x] == [0, 0, 0], background[:, :best_x], image[:, :best_x])\n",
    "                cv2.line(image, (best_x, 0), (best_x, height), (0,0,0), thickness=line_thickness)\n",
    "                required_non_text_percentage = required_non_text_percentage*100\n",
    "                print(f\"Line drawn at x={best_x} with {required_consecutive_columns} consecutive columns with {required_non_text_percentage}% of non-text\")\n",
    "                break\n",
    "        if f == 1:\n",
    "            break\n",
    "    if best_x == -1 or f==0:#\n",
    "#         print(best_x)\n",
    "        print(\"No suitable vertical space found for drawing the line.\")\n",
    "c == f\n",
    "# Save or display the result\n",
    "cv2.imwrite('/kaggle/working/output_image_with_line.png',image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Greyscaling of Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:51:49.303667Z",
     "iopub.status.busy": "2024-12-15T17:51:49.303384Z",
     "iopub.status.idle": "2024-12-15T17:51:49.353621Z",
     "shell.execute_reply": "2024-12-15T17:51:49.352892Z",
     "shell.execute_reply.started": "2024-12-15T17:51:49.303641Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1725285497477,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "5GDC5KPvEfmc",
    "outputId": "7392d4a9-792d-46db-ca43-bd014ea00e16",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pytesseract\n",
    "import cv2\n",
    "image = cv2.imread(\"/kaggle/working/output_image_with_line.png\")\n",
    "\n",
    "# # creating a image object\n",
    "# im1 = cv2.imread(r\"/content/0-0_jpg.rf.91a1b5ecfa392c10b1986ad8070ffb2b11111.jpg\")\n",
    "\n",
    "# # copying image to another image object\n",
    "# im2 = im1.copy()\n",
    "\n",
    "# # shows the copied image\n",
    "# cv2.imwrite(\"/content/combined_gridded.png\",im2)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "cv2.imwrite(\"/kaggle/working/temp_gray.png\",gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Threshold level Merging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:51:49.354828Z",
     "iopub.status.busy": "2024-12-15T17:51:49.354558Z",
     "iopub.status.idle": "2024-12-15T17:51:49.370670Z",
     "shell.execute_reply": "2024-12-15T17:51:49.370023Z",
     "shell.execute_reply.started": "2024-12-15T17:51:49.354803Z"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1725285497477,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "xWdWvuH_FXhK",
    "outputId": "622fbbfe-f0ca-4e46-9f42-1b5f51d4a4e9",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, thresh_black = cv2.threshold(gray, 50, 255, cv2.THRESH_BINARY_INV)\n",
    "_, thresh_white = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)\n",
    "# _, binary_image = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "\n",
    "# Combine both thresholds\n",
    "\n",
    "thresh1 = cv2.bitwise_or(thresh_black, thresh_white)\n",
    "\n",
    "# thresh2 = cv2.threshold(binary_image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)[1]\n",
    "# combined_thresh = cv2.bitwise_or(thresh1, thresh2)\n",
    "\n",
    "cv2.imwrite(\"/kaggle/working/temp_thres_white.png\",thresh1)\n",
    "# cv2.imwrite(\"/kaggle/working/temp_thres_black.png\",thresh2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:53:32.056625Z",
     "iopub.status.busy": "2024-12-15T17:53:32.056284Z",
     "iopub.status.idle": "2024-12-15T17:53:32.156173Z",
     "shell.execute_reply": "2024-12-15T17:53:32.155295Z",
     "shell.execute_reply.started": "2024-12-15T17:53:32.056598Z"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1725285497478,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "jWXy4dSPSEZp",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "449\n",
      "0\n",
      "1250\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "o1 = original_image = cv2.imread('/kaggle/working/2nd_resized_image.jpg')\n",
    "# Load the image\n",
    "image = cv2.imread('/kaggle/working/temp_thres_white.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Threshold the image to get binary image (assuming black is 0 and white is 255)\n",
    "_, binary_image = cv2.threshold(image, 1, 255, cv2.THRESH_BINARY_INV)\n",
    "cv2.imwrite(\"/kaggle/working/binary.jpg\",binary_image)\n",
    "\n",
    "# Find contours in the binary image\n",
    "contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# Initialize a variable to store the minimum x-coordinate of the green line\n",
    "min_x = max_x = min_x_w = original_image.shape[1]  # Start with the width of the image\n",
    "# # Convert grayscale image to BGR to draw colored rectangles\n",
    "# image_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "f = max_y = prev_x = 0\n",
    "# prev_y = prev_x = prev_h = f = 0\n",
    "# Draw red rectangles around the contours\n",
    "# if best_x == -1 or \n",
    "for contour in contours:\n",
    "    x, y, w, h = cv2.boundingRect(contour)\n",
    "    \n",
    "\n",
    "    if h > 500:\n",
    "#         min_x_w =  min(min_x, x+w)\n",
    "#         min_x = min(min_x, x) # Update min_x with the smallest x-coordinate\n",
    "#         max_x = max(max_x,w)\n",
    "#         print(contour)\n",
    "#         max_y = 0# Initialize max_y with the top of the bounding rectangle\n",
    "        for point in contour:\n",
    "            # print(point)\n",
    "            current_y = point[0][1]\n",
    "            if current_y >= max_y and point[0][1]< 1100:\n",
    "                # print(point[0][0])\n",
    "                max_y = current_y\n",
    "                prev_x = point[0][0]\n",
    "#                 w = point[0][0]##########\n",
    "            else:\n",
    "                try:\n",
    "                    # Attempt to access the variable\n",
    "                    print(best_x)\n",
    "                except NameError:\n",
    "                    if point[0][0] < 1100:\n",
    "                        w1 = prev_x\n",
    "                    # Handle the case where the variable is not defined\n",
    "                else:\n",
    "                    if point[0][0] < 1100 or best_x < 1100:\n",
    "                        if best_x != -1 or c!=0:\n",
    "                            w1 = best_x\n",
    "                        else:\n",
    "                            w1 = point[0][0]\n",
    "                        # w = prev_x###############\n",
    "                # print(w)\n",
    "                    f = 1\n",
    "                    break\n",
    "#                    # We've found the highest point, so we can stop searching  \n",
    "    if f==1:\n",
    "            break\n",
    "                \n",
    "min_x = min(min_x, x) # Update min_x with the smallest x-coordinate\n",
    "min_x_w =  min(min_x_w, x+w,w1)\n",
    "# print(w1)\n",
    "# print(min_x_w)\n",
    "max_x = max(max_x,w)\n",
    "cv2.rectangle(original_image, (x, y), (w, max_y), (0, 255, 0), 3)\n",
    "            \n",
    "print(min_x_w)\n",
    "print(min_x)\n",
    "print(max_x)\n",
    "# print(best_x)\n",
    "# Save or display the resulting image\n",
    "if min_x > 500 or min_x_w > min_x:\n",
    "    if min_x < 20:\n",
    "    \n",
    "        min_x = min_x_w\n",
    "    \n",
    "    cropped_image_left = original_image[:,:min_x]\n",
    "    cropped_image_right = original_image[:,min_x:max_x]\n",
    "else:\n",
    "    cropped_image_left = original_image[:, :min_x_w]\n",
    "    cropped_image_right = original_image[:,min_x_w:]\n",
    "# # Save or display the cropped image\n",
    "# plt.imshow(cropped_image_left)\n",
    "# # plt.imshow(cropped_image_right)\n",
    "if cropped_image_left is not None and cropped_image_left.any():\n",
    "    cv2.imwrite('/kaggle/working/cropped_image_left.png', cropped_image_left)\n",
    "    if cropped_image_right is not None and cropped_image_right.any():\n",
    "        cv2.imwrite('/kaggle/working/cropped_image_right.png',cropped_image_right)\n",
    "    else:\n",
    "        print(\"Single column so no right part found!!\")\n",
    "else:\n",
    "    print(\"Single column\")\n",
    "    cv2.imwrite('/kaggle/working/cropped_image_left.png',o1)\n",
    "# plt.axis('off')  # Hide the axis if you just want to display the image\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-13T11:52:31.830269Z",
     "iopub.status.busy": "2024-12-13T11:52:31.830000Z",
     "iopub.status.idle": "2024-12-13T11:52:31.834292Z",
     "shell.execute_reply": "2024-12-13T11:52:31.833422Z",
     "shell.execute_reply.started": "2024-12-13T11:52:31.830243Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working/resume_text137.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-15T17:52:00.987682Z",
     "iopub.status.busy": "2024-12-15T17:52:00.987325Z",
     "iopub.status.idle": "2024-12-15T17:52:09.696544Z",
     "shell.execute_reply": "2024-12-15T17:52:09.695679Z",
     "shell.execute_reply.started": "2024-12-15T17:52:00.987651Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded as API: https://artificialguybr-surya-ocr.hf.space âœ”\n",
      "Loaded as API: https://artificialguybr-surya-ocr.hf.space âœ”\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gradio_client import Client, handle_file\n",
    "\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "left_text=right_text=''\n",
    "\n",
    "left_image = '/kaggle/working/cropped_image_left.png'\n",
    "client = Client(\"artificialguybr/Surya-OCR\")\n",
    "result = client.predict(\n",
    "\t\timage=handle_file(left_image),\n",
    "\t\tlangs=\"en\",\n",
    "\t\tapi_name=\"/ocr_workflow\"\n",
    "    )\n",
    "# print(result)\n",
    "result1= list(result)\n",
    "\n",
    "left_text = result1[-1]\n",
    "# print(left_text)\n",
    "\n",
    "right_image = '/kaggle/working/cropped_image_right.png'\n",
    "if os.path.exists(right_image):\n",
    "    client = Client(\"artificialguybr/Surya-OCR\")\n",
    "    result_r = client.predict(\n",
    "\t\timage=handle_file(right_image),\n",
    "\t\tlangs=\"en\",\n",
    "\t\tapi_name=\"/ocr_workflow\"\n",
    "    )\n",
    "    # print(result)\n",
    "    result2= list(result_r)\n",
    "    right_text = result2[-1]\n",
    "    # print(result1[-1])\n",
    "    \n",
    "with open(\"/kaggle/working/Resume129.txt\", \"a\") as f: ################\n",
    "    fr = ''\n",
    "    fr = fr + str(\"First column: \" )+\"\\n\" \n",
    "    fr = fr + left_text + '\\n'\n",
    "#     for i in range(len(left_text)): \n",
    "#         fr = fr + str(left_text[i]) + \"\\n\" \n",
    "#         # f.write(\"First column: \")\n",
    "\n",
    "    fr1 = ''\n",
    "    fr1 = fr1 + str(\"Second column: \" )+\"\\n\"\n",
    "    fr1 =fr1+ right_text + '\\n' + \"==================================================================\" + \"\\n\"\n",
    "# for i in range(len(right_text)):\n",
    "#     # You were trying to access filtered_non_green_text[0][i] which is incorrect as filtered_non_green_text is a list of strings\n",
    "#     fr1 = fr1 + str(right_text[i]) + \"\\n\"\n",
    "    fr = fr + fr1\n",
    "    f.write(fr)\n",
    "white_page = cv2.imread(\"/kaggle/input/white-page/Screenshot 2024-09-21 043703.png\")\n",
    "cv2.imwrite('/kaggle/working/cropped_image_right.png',white_page)\n",
    "cv2.imwrite('/kaggle/working/cropped_image_left.png',white_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Paddle OCR extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-08T15:06:08.564760Z",
     "iopub.status.busy": "2024-12-08T15:06:08.564131Z",
     "iopub.status.idle": "2024-12-08T15:07:31.139382Z",
     "shell.execute_reply": "2024-12-08T15:07:31.138558Z",
     "shell.execute_reply.started": "2024-12-08T15:06:08.564697Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: OMP_NUM_THREADS set to 3, not 1. The computation speed will not be optimized if you use data parallel. It will fail if this PaddlePaddle binary is compiled with OpenBlas since OpenBlas does not support multi-threads.\n",
      "PLEASE USE OMP_NUM_THREADS WISELY.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://paddleocr.bj.bcebos.com/PP-OCRv3/english/en_PP-OCRv3_det_infer.tar to /root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer/en_PP-OCRv3_det_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3910/3910 [00:19<00:00, 202.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://paddleocr.bj.bcebos.com/PP-OCRv4/english/en_PP-OCRv4_rec_infer.tar to /root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer/en_PP-OCRv4_rec_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10000/10000 [00:18<00:00, 527.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download https://paddleocr.bj.bcebos.com/dygraph_v2.0/ch/ch_ppocr_mobile_v2.0_cls_infer.tar to /root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer/ch_ppocr_mobile_v2.0_cls_infer.tar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2138/2138 [00:15<00:00, 134.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/12/08 15:07:10] ppocr DEBUG: Namespace(help='==SUPPRESS==', use_gpu=False, use_xpu=False, use_npu=False, use_mlu=False, ir_optim=True, use_tensorrt=False, min_subgraph_size=15, precision='fp32', gpu_mem=500, gpu_id=0, image_dir=None, page_num=0, det_algorithm='DB', det_model_dir='/root/.paddleocr/whl/det/en/en_PP-OCRv3_det_infer', det_limit_side_len=960, det_limit_type='max', det_box_type='quad', det_db_thresh=0.3, det_db_box_thresh=0.6, det_db_unclip_ratio=1.5, max_batch_size=10, use_dilation=False, det_db_score_mode='fast', det_east_score_thresh=0.8, det_east_cover_thresh=0.1, det_east_nms_thresh=0.2, det_sast_score_thresh=0.5, det_sast_nms_thresh=0.2, det_pse_thresh=0, det_pse_box_thresh=0.85, det_pse_min_area=16, det_pse_scale=1, scales=[8, 16, 32], alpha=1.0, beta=1.0, fourier_degree=5, rec_algorithm='SVTR_LCNet', rec_model_dir='/root/.paddleocr/whl/rec/en/en_PP-OCRv4_rec_infer', rec_image_inverse=True, rec_image_shape='3, 48, 320', rec_batch_num=6, max_text_length=25, rec_char_dict_path='/opt/conda/lib/python3.10/site-packages/paddleocr/ppocr/utils/en_dict.txt', use_space_char=True, vis_font_path='./doc/fonts/simfang.ttf', drop_score=0.5, e2e_algorithm='PGNet', e2e_model_dir=None, e2e_limit_side_len=768, e2e_limit_type='max', e2e_pgnet_score_thresh=0.5, e2e_char_dict_path='./ppocr/utils/ic15_dict.txt', e2e_pgnet_valid_set='totaltext', e2e_pgnet_mode='fast', use_angle_cls=True, cls_model_dir='/root/.paddleocr/whl/cls/ch_ppocr_mobile_v2.0_cls_infer', cls_image_shape='3, 48, 192', label_list=['0', '180'], cls_batch_num=6, cls_thresh=0.9, enable_mkldnn=False, cpu_threads=10, use_pdserving=False, warmup=False, sr_model_dir=None, sr_image_shape='3, 32, 128', sr_batch_num=1, draw_img_save_dir='./inference_results', save_crop_res=False, crop_res_save_dir='./output', use_mp=False, total_process_num=1, process_id=0, benchmark=False, save_log_path='./log_output/', show_log=True, use_onnx=False, return_word_box=False, output='./output', table_max_len=488, table_algorithm='TableAttn', table_model_dir=None, merge_no_span_structure=True, table_char_dict_path=None, formula_algorithm='LaTeXOCR', formula_model_dir=None, formula_char_dict_path=None, formula_batch_num=1, layout_model_dir=None, layout_dict_path=None, layout_score_threshold=0.5, layout_nms_threshold=0.5, kie_algorithm='LayoutXLM', ser_model_dir=None, re_model_dir=None, use_visual_backbone=True, ser_dict_path='../train_data/XFUND/class_list_xfun.txt', ocr_order_method=None, mode='structure', image_orientation=False, layout=True, table=True, formula=False, ocr=True, recovery=False, recovery_to_markdown=False, use_pdf2docx_api=False, invert=False, binarize=False, alphacolor=(255, 255, 255), lang='en', det=True, rec=True, type='ocr', savefile=False, ocr_version='PP-OCRv4', structure_version='PP-StructureV2')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024/12/08 15:07:11] ppocr DEBUG: dt_boxes num : 45, elapsed : 0.17926549911499023\n",
      "[2024/12/08 15:07:11] ppocr DEBUG: cls num  : 45, elapsed : 0.1522059440612793\n",
      "[2024/12/08 15:07:17] ppocr DEBUG: rec_res num  : 45, elapsed : 5.714731931686401\n",
      "[2024/12/08 15:07:18] ppocr DEBUG: dt_boxes num : 51, elapsed : 0.3309452533721924\n",
      "[2024/12/08 15:07:18] ppocr DEBUG: cls num  : 51, elapsed : 0.10141253471374512\n",
      "[2024/12/08 15:07:31] ppocr DEBUG: rec_res num  : 51, elapsed : 12.962459564208984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from paddleocr import PaddleOCR\n",
    "import cv2\n",
    "import os\n",
    "ocr = PaddleOCR(use_angle_cls=True,lang='en')\n",
    "\n",
    "left_text=right_text=''\n",
    "\n",
    "# fixed_size = (1250,1563)\n",
    "# l_image = cv2.imread(\"/kaggle/working/cropped_image_left.png\")\n",
    "# image = cv2.resize(l_image, fixed_size)\n",
    "# cv2.imwrite(\"/kaggle/working/2nd_resized_image.jpg\",image)\n",
    "# r_image = cv2.imread(\"/kaggle/working/cropped_image_right.png\")\n",
    "# image = cv2.resize(r_image, fixed_size)\n",
    "# cv2.imwrite(\"/kaggle/working/2nd_resized_image.jpg\",image)\n",
    "\n",
    "left_image = '/kaggle/working/cropped_image_left.png'\n",
    "result = ocr.ocr(left_image)\n",
    "for line in result:\n",
    "    if line is not None:\n",
    "        for word_info in line:\n",
    "            text = word_info[1][0]\n",
    "            if text is not None:\n",
    "    #             print(text)\n",
    "                left_text = left_text + text + '\\n'\n",
    "# print(left_text)\n",
    "\n",
    "right_image = '/kaggle/working/cropped_image_right.png'\n",
    "if os.path.exists(right_image):\n",
    "    result = ocr.ocr(right_image, cls=True)\n",
    "    for line in result:\n",
    "        if line is not None:\n",
    "            for word_info in line:\n",
    "                text = word_info[1][0]\n",
    "                if text is not None:\n",
    "                    right_text = right_text + text + '\\n'\n",
    "with open(\"/kaggle/working/cv_new.txt\", \"a\") as f: ################\n",
    "    fr = ''\n",
    "    fr = fr + str(\"First column: \" )+\"\\n\" \n",
    "    fr = fr + left_text + '\\n'\n",
    "#     for i in range(len(left_text)): \n",
    "#         fr = fr + str(left_text[i]) + \"\\n\" \n",
    "#         # f.write(\"First column: \")\n",
    "\n",
    "    fr1 = ''\n",
    "    fr1 = fr1 + str(\"Second column: \" )+\"\\n\"\n",
    "    fr1 =fr1+ right_text + '\\n' + \"==================================================================\" + \"\\n\"\n",
    "# for i in range(len(right_text)):\n",
    "#     # You were trying to access filtered_non_green_text[0][i] which is incorrect as filtered_non_green_text is a list of strings\n",
    "#     fr1 = fr1 + str(right_text[i]) + \"\\n\"\n",
    "    fr = fr + fr1\n",
    "    f.write(fr)\n",
    "white_page = cv2.imread(\"/kaggle/input/white-page/Screenshot 2024-09-21 043703.png\")\n",
    "cv2.imwrite('/kaggle/working/cropped_image_right.png',white_page)\n",
    "cv2.imwrite('/kaggle/working/cropped_image_left.png',white_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1725285496764,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "Lz2YOkIOoarJ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the image\n",
    "# image = cv2.imread('/content/6db3beabacf07d2ef8dfe3602c8a7914_png.rf.7f123796ce4d47c264b21327b04b9712_page-0001.jpg')\n",
    "# # Convert to grayscale\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # Apply binary thresholding to invert the image (text becomes white, background becomes black)\n",
    "# _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# # Find contours\n",
    "# contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Sort contours based on the x-coordinate (to identify the columns)\n",
    "# contours = sorted(contours, key=lambda c: cv2.boundingRect(c)[0])\n",
    "# cv2.drawContours(image, contours, -1, (0, 255, 0), 2)\n",
    "# plt.imshow(image)\n",
    "# plt.show\n",
    "# # Assume there are two main contours (columns), find their bounding rectangles\n",
    "# left_col = cv2.boundingRect(contours[0])\n",
    "# right_col = cv2.boundingRect(contours[-1])\n",
    "\n",
    "# # Calculate the middle of the space between the two columns\n",
    "# print(\"L0\",left_col[0],\"L2\",left_col[2])\n",
    "# print(\"R0\",right_col[0],\"R2\",right_col[2])\n",
    "# ############################Solve below line..\n",
    "# middle_x = (max(right_col[0],right_col[2])-((left_col[2]//(right_col[2])) * (1+left_col[0]) )-min(left_col[0],left_col[2]))  // 2\n",
    "# # #- ((max(left_col[0],left_col[2]) + 2 * min(left_col[0],left_col[2]))\n",
    "# # middle_x = (  right_col[0] + left_col[0] + left_col[2])  // 2\n",
    "# # Draw the vertical line at the middle of the space\n",
    "# cv2.line(image, (middle_x, 0), (middle_x, image.shape[0]), (0, 0, 0), 2)\n",
    "\n",
    "# # Save or display the result\n",
    "# cv2.imwrite('document_with_divider.png', image)\n",
    "\n",
    "# # #-----------------------------------------------------\n",
    "# # # Calculate the middle x-coordinate between the two columns\n",
    "# # middle_x = (left_col[0] + left_col[2] + right_col[0]) // 2\n",
    "\n",
    "# # # Draw the vertical line at the middle of the space from top to bottom\n",
    "# # cv2.line(image, (middle_x, 0), (middle_x, image.shape[0]), (0, 0, 0), 2)\n",
    "\n",
    "# # # Save or display the result\n",
    "# # cv2.imwrite('/mnt/data/document_with_divider.png', image)\n",
    "\n",
    "# # # or\n",
    "# # # plt.imshow('Image with Divider', image)\n",
    "# # # plt.show()\n",
    "# # cv2.waitKey(0)\n",
    "# # cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1725285496764,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "L5D8Tq9hucet",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the image\n",
    "# image = cv2.imread('/content/6db3beabacf07d2ef8dfe3602c8a7914_png.rf.7f123796ce4d47c264b21327b04b9712_page-0001.jpg')\n",
    "\n",
    "# # Convert the image to grayscale\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # Apply binary thresholding\n",
    "# _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# # Find the contours\n",
    "# contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Create a mask for drawing\n",
    "# mask = np.zeros_like(binary)\n",
    "\n",
    "# # Draw the contours on the mask\n",
    "# cv2.drawContours(mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "# # Invert the mask to get the spaces\n",
    "# mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "# # Sum up the columns of the mask_inv to get the column densities\n",
    "# column_densities = np.sum(mask_inv, axis=0)\n",
    "\n",
    "# # Find the index with the lowest density which should be the vertical space between columns\n",
    "# min_index = np.argmin(column_densities)\n",
    "\n",
    "# # Draw a vertical line at the min_index position\n",
    "# line_thickness = 2\n",
    "# cv2.line(image, (min_index, 0), (min_index, image.shape[0]), (0, 0, 255), thickness=line_thickness)\n",
    "\n",
    "# # Save or display the result\n",
    "# cv2.imwrite('output_image_with_line.png', image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1725285496764,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "6fMaX5NHwQgr",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the image\n",
    "# image = cv2.imread('/content/5f5a93dc-4946-425b-8d44-17fcf8ba3471-1_jpg.rf.440f1b75afa3cc5f782eeb5aeb0956d4_page-0001.jpg')\n",
    "\n",
    "# # Convert the image to grayscale\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # Apply binary thresholding to get a binary image\n",
    "# _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# # Find the contours of the text regions\n",
    "# contours, _ = cv2.findContours(binary, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Create a mask for the text regions\n",
    "# mask = np.zeros_like(binary)\n",
    "\n",
    "# # Draw filled contours on the mask (text regions become white)\n",
    "# cv2.drawContours(mask, contours, -1, 255, thickness=cv2.FILLED)\n",
    "\n",
    "# # Invert the mask to get the non-text areas (non-text areas become white)\n",
    "# mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "# # Start searching for the vertical line position\n",
    "# height, width = mask_inv.shape\n",
    "# best_x = -1\n",
    "# max_non_text_percentage = 0.8  # 80% non-text space requirement\n",
    "# flag = 0\n",
    "\n",
    "# # Scan across the x-axis to find the best position\n",
    "# for x in range(width):\n",
    "#     # Calculate the percentage of non-text pixels along the y-axis at this x position\n",
    "#     non_text_pixels = np.sum(mask_inv[:, x]) / 255\n",
    "#     non_text_percentage = non_text_pixels / height\n",
    "\n",
    "#     # Check if this position meets the 80% non-text requirement\n",
    "#     if non_text_percentage >= max_non_text_percentage:\n",
    "#       if x == 0:\n",
    "#         continue\n",
    "#       else:\n",
    "#         best_x = x\n",
    "#         break\n",
    "\n",
    "# # Draw the vertical line at the found position if it exists\n",
    "# if best_x != -1:\n",
    "#     line_thickness = 2\n",
    "#     cv2.line(image, (best_x, 0), (best_x, height), (0, 0, 255), thickness=line_thickness)\n",
    "#     print(f\"Line drawn at x={best_x} where non-text percentage is {non_text_percentage*100:.2f}%\")\n",
    "# else:\n",
    "#     print(\"No suitable vertical space found for drawing the line.\")\n",
    "\n",
    "# # Save or display the result\n",
    "# cv2.imwrite('/content/output_image_with_line.png', image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 717,
     "status": "ok",
     "timestamp": 1725285497477,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "qhLhiWMeMxfG",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Function to find the largest vertical gap in a column\n",
    "# def find_largest_vertical_gap(binary_column):\n",
    "#     max_gap = 0\n",
    "#     max_start = 0\n",
    "#     gap_start = -1\n",
    "#     in_gap = False\n",
    "\n",
    "#     for i, pixel in enumerate(binary_column):\n",
    "#         if pixel == 0:  # If pixel is black (object)\n",
    "#             if in_gap:\n",
    "#                 gap_size = i - gap_start\n",
    "#                 if gap_size > max_gap:\n",
    "#                     max_gap = gap_size\n",
    "#                     max_start = gap_start\n",
    "#                 in_gap = False\n",
    "#         else:  # If pixel is white (background)\n",
    "#             if not in_gap:\n",
    "#                 gap_start = i\n",
    "#                 in_gap = True\n",
    "\n",
    "#     # Check the last gap\n",
    "#     if in_gap:\n",
    "#         gap_size = len(binary_column) - gap_start\n",
    "#         if gap_size > max_gap:\n",
    "#             max_gap = gap_size\n",
    "#             max_start = gap_start\n",
    "\n",
    "#     return max_start, max_start + max_gap\n",
    "\n",
    "# # Load the image\n",
    "# image = cv2.imread('/content/6db3beabacf07d2ef8dfe3602c8a7914_png.rf.7f123796ce4d47c264b21327b04b9712_page-0001.jpg')\n",
    "# height, width = image.shape[:2]\n",
    "\n",
    "# # Convert the image to grayscale\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # Apply a threshold to get a binary image\n",
    "# _, binary = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# # Initialize variables to store the column with the largest gap\n",
    "# largest_gap_column = 0\n",
    "# largest_gap_start = 0\n",
    "# largest_gap_end = 0\n",
    "# largest_gap_size = 0\n",
    "\n",
    "# # Iterate over each column to find the one with the largest vertical gap\n",
    "# for col in range(width):\n",
    "#     start, end = find_largest_vertical_gap(binary[:, col])\n",
    "#     gap_size = end - start\n",
    "#     if gap_size > largest_gap_size:\n",
    "#         largest_gap_size = gap_size\n",
    "#         largest_gap_start = start\n",
    "#         largest_gap_end = end\n",
    "#         largest_gap_column = col\n",
    "\n",
    "# # Draw a line at the center of the largest vertical gap\n",
    "# if largest_gap_size > 0:\n",
    "#     center_y = (largest_gap_start + largest_gap_end) // 2\n",
    "#     cv2.line(image, (largest_gap_column - 10, center_y), (largest_gap_column + 10, center_y), (0, 255, 0), 2)\n",
    "\n",
    "# # Display the result\n",
    "# cv2.imwrite('/content/document_with_divider.png', image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1725285497477,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "OZfcO_9vGUYY",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# image = cv2.imread(\"/content/5efae9e3b8c75913e210bf72_example-real-estate-agent-resume_png.rf.6c223485926112f23ca452c0f9c76e2b_page-0001.jpg\")\n",
    "# image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:28:23.515629Z",
     "iopub.status.busy": "2024-09-02T17:28:23.514854Z",
     "iopub.status.idle": "2024-09-02T17:28:23.661944Z",
     "shell.execute_reply": "2024-09-02T17:28:23.660863Z",
     "shell.execute_reply.started": "2024-09-02T17:28:23.515587Z"
    },
    "executionInfo": {
     "elapsed": 748,
     "status": "ok",
     "timestamp": 1725285498223,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "d6TAYfuiohpr",
    "outputId": "068e02de-897e-4df5-bd0a-0a087b37b515"
   },
   "source": [
    "image1 = cv2.imread('/kaggle/working/original_with_green_boxes1.png')\n",
    "image2 = cv2.imread('/kaggle/working/original_with_green_boxes2.png')\n",
    "final_img = cv2.bitwise_or(image1, image2)\n",
    "cv2.imwrite(\"/kaggle/working/final_img_with_green_boxes.png\",final_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-02T17:28:30.680864Z",
     "iopub.status.busy": "2024-09-02T17:28:30.680465Z",
     "iopub.status.idle": "2024-09-02T17:28:49.177615Z",
     "shell.execute_reply": "2024-09-02T17:28:49.176729Z",
     "shell.execute_reply.started": "2024-09-02T17:28:30.680825Z"
    },
    "executionInfo": {
     "elapsed": 23331,
     "status": "error",
     "timestamp": 1725285734071,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "aI4yZ0BuWVpe",
    "outputId": "a35358c4-4827-41f9-cd4c-4a25df8afc6a"
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from paddleocr import PaddleOCR\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Load the image\n",
    "# image_path = '/kaggle/working/original_with_green_boxes1.png'\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# # Convert image to HSV color space for color detection\n",
    "# hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# # Define the range for green color in HSV space\n",
    "# lower_green = np.array([35, 100, 100])  # Lower bound of green color\n",
    "# upper_green = np.array([85, 255, 255])  # Upper bound of green color\n",
    "\n",
    "# # Create a mask for green color\n",
    "# mask = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "\n",
    "# # Find contours of the green boxes\n",
    "# contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Create a mask to differentiate between green and non-green areas\n",
    "# non_green_mask = cv2.bitwise_not(mask)\n",
    "# # Initialize PaddleOCR\n",
    "# ocr = PaddleOCR(use_angle_cls=True, lang='en')  # Enable angle classification and set language to English\n",
    "\n",
    "# def get_text_from_mask(image, mask):\n",
    "#     \"\"\"Extract text from the image using a given mask.\"\"\"\n",
    "#     masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "#     results = ocr.ocr(masked_image, cls=True)\n",
    "#     return results\n",
    "\n",
    "# # Process green boxes\n",
    "# roi_texts = []\n",
    "# for contour in contours:\n",
    "#     x, y, w, h = cv2.boundingRect(contour)\n",
    "#     roi_mask = np.zeros_like(mask)\n",
    "#     cv2.drawContours(roi_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "#     text = get_text_from_mask(image, roi_mask)\n",
    "#     roi_texts.append(text)\n",
    "\n",
    "# # Process non-green areas\n",
    "# non_green_text = get_text_from_mask(image, non_green_mask)\n",
    "\n",
    "# # Function to filter text\n",
    "# def filter_text_only(results):\n",
    "#     text_only = []\n",
    "#     for res in results:\n",
    "#         if res:\n",
    "#             for line in res:\n",
    "#               ###############################################Problem else all done\n",
    "#                 text_only.append(line[-1][0])  # Extract the text from the result\n",
    "#         return text_only\n",
    "#     # for res in results:\n",
    "#     #     text = res[1]\n",
    "#     #     # Use regex to check if the string contains any numerical characters\n",
    "#     #     if not re.search(r'\\d', text):\n",
    "#     #         text_only.append(text)\n",
    "#     # return text_only\n",
    "\n",
    "# # Apply the filter to each ROI's OCR results\n",
    "# filtered_roi_texts = [filter_text_only(roi_text) for roi_text in roi_texts]\n",
    "# filtered_non_green_text = filter_text_only(non_green_text)\n",
    "\n",
    "# # Write all the text to the file within a single 'with' block\n",
    "# with open(\"/kaggle/working/resume_text2.txt\", \"a\") as f:\n",
    "#     fr = ''\n",
    "#     fr = fr + str(\"First column: \" )+\"\\n\"\n",
    "#     for i in range(len(filtered_roi_texts[0])):\n",
    "#         fr = fr + str(filtered_roi_texts[0][i]) + \"\\n\"\n",
    "#     # f.write(\"First column: \")\n",
    "\n",
    "\n",
    "#     fr1 = ''\n",
    "#     fr1 = fr1 + str(\"Second column: \" )+\"\\n\"\n",
    "#   # Added a newline character here for better formatting\n",
    "#     for i in range(len(filtered_non_green_text)):\n",
    "#         # You were trying to access filtered_non_green_text[0][i] which is incorrect as filtered_non_green_text is a list of strings\n",
    "#         fr1 = fr1 + str(filtered_non_green_text[i]) + \"\\n\"\n",
    "#     fr = fr + fr1\n",
    "#     f.write(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1725285497478,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "wCbw007kSajl",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the image\n",
    "# original_image = cv2.imread('/kaggle/input/r123456/5efae9e3b8c75913e210bf72_example-real-estate-agent-resume_png.rf.6c223485926112f23ca452c0f9c76e2b_page-0001.jpg')\n",
    "# image = cv2.imread('/kaggle/working/temp_thres_black.png', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# # Threshold the image to get binary image (assuming black is 0 and white is 255)\n",
    "# _, binary_image = cv2.threshold(image, 1, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "# # Find contours in the binary image\n",
    "# contours, _ = cv2.findContours(binary_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Convert grayscale image to BGR to draw colored rectangles\n",
    "# image_bgr = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "# # Draw red rectangles around the contours\n",
    "# for contour in contours:\n",
    "#     x, y, w, h = cv2.boundingRect(contour)\n",
    "#     if h > 100:\n",
    "#       cv2.rectangle(original_image, (x, y), (x+w, y+h), (0,255, 0), 3)\n",
    "\n",
    "\n",
    "# # Save or display the resulting image\n",
    "# cv2.imwrite('/kaggle/working/original_with_green_boxes2.png', original_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1725285498223,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "t3N7oOGX1wZ-",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import easyocr\n",
    "# import numpy as np\n",
    "# import re\n",
    "\n",
    "# # Load the image\n",
    "# image_path = '/content/final_img_with_green_boxes.png'\n",
    "# image = cv2.imread(image_path)\n",
    "\n",
    "# # Convert image to HSV color space for color detection\n",
    "# hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# # Define the range for green color in HSV space\n",
    "# lower_green = np.array([35, 100, 100])  # Lower bound of green color\n",
    "# upper_green = np.array([85, 255, 255])  # Upper bound of green color\n",
    "\n",
    "# # Create a mask for green color\n",
    "# mask = cv2.inRange(hsv_image, lower_green, upper_green)\n",
    "\n",
    "# # Find contours of the green boxes\n",
    "# contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Create a mask to differentiate between green and non-green areas\n",
    "# non_green_mask = cv2.bitwise_not(mask)\n",
    "\n",
    "# # Initialize EasyOCR Reader\n",
    "# reader = easyocr.Reader(['en'])\n",
    "\n",
    "# def get_text_from_mask(image, mask):\n",
    "#     \"\"\"Extract text from the image using a given mask.\"\"\"\n",
    "#     masked_image = cv2.bitwise_and(image, image, mask=mask)\n",
    "#     results = reader.readtext(masked_image)\n",
    "#     return results\n",
    "\n",
    "# # Process green boxes\n",
    "# roi_texts = []\n",
    "# for contour in contours:\n",
    "#     x, y, w, h = cv2.boundingRect(contour)\n",
    "#     roi_mask = np.zeros_like(mask)\n",
    "#     cv2.drawContours(roi_mask, [contour], -1, 255, thickness=cv2.FILLED)\n",
    "#     text = get_text_from_mask(image, roi_mask)\n",
    "#     roi_texts.append(text)\n",
    "\n",
    "# # Process non-green areas\n",
    "# non_green_text = get_text_from_mask(image, non_green_mask)\n",
    "\n",
    "# # Function to filter out numerical values\n",
    "# def filter_text_only(results):\n",
    "#     text_only = []\n",
    "#     for res in results:\n",
    "#         text = res[1]\n",
    "\n",
    "#         text_only.append(text)\n",
    "#     return text_only\n",
    "\n",
    "# # Apply the filter to each ROI's OCR results\n",
    "# filtered_roi_texts = [filter_text_only(roi_text) for roi_text in roi_texts]\n",
    "# filtered_non_green_text = filter_text_only(non_green_text)\n",
    "\n",
    "# # Print the filtered text\n",
    "# # print(\"Filtered Text from Green Boxes:\", filtered_roi_texts)\n",
    "# # print(\"Filtered Text from Non-Green Areas:\", filtered_non_green_text)\n",
    "# fr = ''\n",
    "# for i in range(len(filtered_roi_texts[0])):\n",
    "#   fr = fr + str(filtered_roi_texts[0][i]) + \" \"\n",
    "# f = open(\"/content/resume_text.txt\", \"a\")\n",
    "# f.write(fr)\n",
    "# f.close()\n",
    "# print(filtered_non_green_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 682,
     "status": "ok",
     "timestamp": 1725285705345,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "hVvVL0DONb4_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "  # import cv2\n",
    "# import easyocr\n",
    "\n",
    "# # Load the image\n",
    "# image = cv2.imread('/content/final_img_with_green_boxes.png')\n",
    "\n",
    "# # Initialize the EasyOCR reader\n",
    "# reader = easyocr.Reader(['en'])\n",
    "\n",
    "# # Perform OCR to detect text\n",
    "# results = reader.readtext(image)\n",
    "# # prev_h = 0\n",
    "# # prev_bottom_right = prev_top_left = (0,0)\n",
    "# # Iterate through the detected text and extract dimensions\n",
    "# for (bbox, text, prob) in results:\n",
    "#     # bbox: bounding box coordinates [(top_left), (top_right), (bottom_right), (bottom_left)]\n",
    "#     # text: the recognized text\n",
    "#     # prob: the probability of the recognition\n",
    "\n",
    "#     # Unpack bounding box coordinates\n",
    "#     # (top_left, top_right, bottom_right, bottom_left) = bbox\n",
    "\n",
    "#     # # Convert the coordinates to integers\n",
    "#     # top_left = tuple(map(int, top_left))\n",
    "#     # bottom_right = tuple(map(int, bottom_right))\n",
    "\n",
    "#     # # Calculate dimensions\n",
    "#     # w = bottom_right[0] - top_left[0]\n",
    "#     # h = bottom_right[1] - top_left[1]\n",
    "#     print(f\"{text}\")\n",
    "#     # print(\"top_left:\",top_left,\"bottom_right:\",bottom_right,\"width:\",w,\"Height:\",h)\n",
    "#     # cv2.rectangle(image, top_left, bottom_right, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "# # Display the image with bounding boxes\n",
    "# # cv2.imwrite('/content/dimension1.jpg',image)\n",
    "\n",
    "\n",
    "# #\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbF7C0t2eHnw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "# # Load the original image and the image with green boxes\n",
    "# original_image = cv2.imread('/content/0-0_jpg.rf.91a1b5ecfa392c10b1986ad8070ffb2b11111.jpg')  # Path to the original image\n",
    "# image_with_boxes = cv2.imread('/content/output_image.jpg')  # Path to the image with green boxes\n",
    "\n",
    "# # Convert the image with green boxes to HSV (Hue, Saturation, Value) color space\n",
    "# hsv = cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# # Define the range for green color in HSV space\n",
    "# lower_green = np.array([40, 40, 40])  # Lower bound of green color\n",
    "# upper_green = np.array([80, 255, 255])  # Upper bound of green color\n",
    "\n",
    "# # Create a mask where green colors are white and other colors are black\n",
    "# mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "# # Isolate the green boxes from the image with boxes\n",
    "# green_boxes = cv2.bitwise_or(image_with_boxes, image_with_boxes, mask = mask )\n",
    "# plt.imshow(green_boxes)\n",
    "# plt.show()\n",
    "# # Add the green boxes to the original image\n",
    "# result = cv2.addWeighted(original_image, 1, green_boxes[1], 1, 0)\n",
    "\n",
    "# # Save or display the result\n",
    "# cv2.imwrite('/content/original_with_green_boxes.png', result)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LXrXQzFrTnbw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the original image and the image with green boxes\n",
    "# original_image = cv2.imread('/content/0-0_jpg.rf.91a1b5ecfa392c10b1986ad8070ffb2b11111.jpg')  # Path to the original image\n",
    "# image_with_boxes = cv2.imread('/content/output_image.png')  # Path to the image with green boxes\n",
    "\n",
    "# # Convert the image with green boxes to HSV (Hue, Saturation, Value) color space\n",
    "# hsv = cv2.cvtColor(image_with_boxes, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# # Define the range for green color in HSV space\n",
    "# lower_green = np.array([40, 40, 40])  # Lower bound of green color\n",
    "# upper_green = np.array([80, 255, 255])  # Upper bound of green color\n",
    "\n",
    "# # Create a mask where green colors are white and other colors are black\n",
    "# mask = cv2.inRange(hsv, lower_green, upper_green)\n",
    "\n",
    "# # Invert the mask to get the non-green areas (green areas become black, others become white)\n",
    "# mask_inv = cv2.bitwise_not(mask)\n",
    "\n",
    "# # Use the inverted mask to extract the non-green parts from the image with boxes\n",
    "# non_green_parts = cv2.bitwise_and(image_with_boxes, image_with_boxes, mask=mask_inv)\n",
    "\n",
    "# # Use the mask to extract the green parts from the original image\n",
    "# green_parts = cv2.bitwise_and(original_image, original_image, mask=mask)\n",
    "\n",
    "# # Combine the non-green parts of the image with boxes with the green parts from the original image\n",
    "# result = cv2.add(non_green_parts, green_parts)\n",
    "\n",
    "# # Save or display the result\n",
    "# cv2.imwrite('/content/output_image.jpg', result)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FOZE3xFuG5s8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cnts = cv2.findContours(thresh1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "# cnts = sorted(cnts, key=lambda x: cv2.boundingRect(x)[0])\n",
    "# color = (0, 255, 0)  # Green color\n",
    "# tw = 0\n",
    "# start_h = 0\n",
    "# start_x = start_y = 0\n",
    "# for index,c in enumerate(cnts):\n",
    "#   x,y,w,h = cv2.boundingRect(c)\n",
    "#   if index != 0:\n",
    "#     print(\"y:\",y,\"h:\",h,\"x:\",x)\n",
    "#     # if start_h >= h :\n",
    "#     image[y:y+h, x:x+w+19] = color\n",
    "\n",
    "# #         tw = tw + w\n",
    "# #     else:\n",
    "# #         cv2.rectangle(image, (start_x, y), (start_x + tw, y + h), color, 2)\n",
    "# #         start_h = h\n",
    "# #         start_x = x\n",
    "# #     # print(\"y:\",y,\"prev_h:\",prev_h,\"start_h:\",start_h,\"start_x\",start_x)\n",
    "# #   #     if flag == 0 and prev_h < h :\n",
    "# #   #       flag = 1\n",
    "# #   #       break\n",
    "# #   #     else:\n",
    "# #   #       flag = 0\n",
    "\n",
    "# #     # image[y:y+h+int(average_height), x:x+w+int(average_width)] = color\n",
    "# # # cnts = cv2.findContours(thresh1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# # # cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "# # # cnts = sorted(cnts, key=lambda x: cv2.boundingRect(x)[0])\n",
    "\n",
    "# # # color = (0, 255, 0)  # Green color\n",
    "# # # start_y = 0\n",
    "# # # prev_h = 0\n",
    "# # # prev_y = 0\n",
    "# # # for index, c in enumerate(cnts):\n",
    "# # #       x, y, w, h = cv2.boundingRect(c)\n",
    "# # #       print(\"y\",y,\"h\",h,\"ph:\",prev_h,\"sy\",start_y,\"py:\",prev_y)\n",
    "# # #       if index != 0:\n",
    "# # #           if prev_h < h:\n",
    "# # #             # if prev_h != 0:\n",
    "# # #             cv2.rectangle(image, (x, start_y), (x + w, prev_y + prev_h), color, 2)\n",
    "# # #             start_y = prev_y + prev_h\n",
    "# # #             prev_h = h\n",
    "# # #             prev_y = y\n",
    "# # #           else:\n",
    "# # #             if prev_h >= h:\n",
    "# # #               prev_h = h\n",
    "# # #               prev_y = y\n",
    "# # #             # cv2.rectangle(image, (x, start_y), (x + w, prev_y + prev_h), (0, 0, 255), 2)\n",
    "# # #           # prev_h = h\n",
    "# # #           if index == 20:\n",
    "# # #             # cv2.rectangle(image, (x, start_y), (x + w, prev_y + prev_h), color, 2)\n",
    "# # #             break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# cv2.imwrite(\"/content/temp_rect1.png\",image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eyA3FKjOD__H",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the image\n",
    "# # image = cv2.imread('/content/temp_rect1.png')\n",
    "# image = cv2.imread('/content/temp_thres_white.png')\n",
    "# # Convert the image to grayscale\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # # Threshold to detect green regions\n",
    "# # lower_green = np.array([0, 150, 0])\n",
    "# # upper_green = np.array([0, 255, 0])\n",
    "# # mask_green = cv2.inRange(image, lower_green, upper_green)\n",
    "\n",
    "# # # Find contours in the green regions\n",
    "# # contours, _ = cv2.findContours(mask_green, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# contours = cv2.findContours(thresh1, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "# contours = sorted(contours, key=lambda x: cv2.boundingRect(x)[0])\n",
    "\n",
    "# # Separate contours into left and right columns\n",
    "# left_contours = []\n",
    "# right_contours = []\n",
    "\n",
    "# image_center = image.shape[1] // 2\n",
    "\n",
    "# for cnt in contours:\n",
    "#     x, y, w, h = cv2.boundingRect(cnt)\n",
    "#     if x < image_center:\n",
    "#         left_contours.append((x, y, w, h))\n",
    "#     else:\n",
    "#         right_contours.append((x, y, w, h))\n",
    "\n",
    "# # Draw right column contours first\n",
    "# for (x, y, w, h) in right_contours:\n",
    "#     cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)  # Red color\n",
    "\n",
    "# # Draw left column contours and adjust them if they intersect with right column boxes\n",
    "# for (lx, ly, lw, lh) in left_contours:\n",
    "#     for (rx, ry, rw, rh) in right_contours:###########################\n",
    "#         if lx + lw > rx and ly < ry + rh and ly + lh > ry:  # Check for intersection\n",
    "#             lw = rx - lx  # Adjust left contour width to stop at right contour\n",
    "\n",
    "#     cv2.rectangle(image, (lx, ly), (lx + lw, ly + lh), (0, 0, 255), 2)  # Red color\n",
    "\n",
    "# # Display the image with modified rectangles\n",
    "# cv2.imwrite(\"/content/temp_gridded_c1.png\",image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYeqK2zdFbdo",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the image\n",
    "# # image = cv2.imread('/content/temp_rect1.png')\n",
    "# image = cv2.imread('/content/temp_thres_black.png')\n",
    "# # Convert the image to grayscale\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # # Threshold to detect green regions\n",
    "# # lower_green = np.array([0, 150, 0])\n",
    "# # upper_green = np.array([0, 255, 0])\n",
    "# # mask_green = cv2.inRange(image, lower_green, upper_green)\n",
    "\n",
    "# # # Find contours in the green regions\n",
    "# # contours, _ = cv2.findContours(mask_green, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# contours = cv2.findContours(thresh2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# contours = contours[0] if len(contours) == 2 else contours[1]\n",
    "# contours = sorted(contours, key=lambda x: cv2.boundingRect(x)[0])\n",
    "\n",
    "# # Separate contours into left and right columns\n",
    "# left_contours = []\n",
    "# right_contours = []\n",
    "\n",
    "# image_center = image.shape[1] // 2\n",
    "\n",
    "# for cnt in contours:\n",
    "#     x, y, w, h = cv2.boundingRect(cnt)\n",
    "#     if x < image_center:\n",
    "#         left_contours.append((x, y, w, h))\n",
    "#     else:\n",
    "#         right_contours.append((x, y, w, h))\n",
    "\n",
    "# # Draw right column contours first\n",
    "# for (x, y, w, h) in right_contours:\n",
    "#     cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)  # Red color\n",
    "\n",
    "# # Draw left column contours and adjust them if they intersect with right column boxes\n",
    "# for (lx, ly, lw, lh) in left_contours:\n",
    "#     for (rx, ry, rw, rh) in right_contours:###########################\n",
    "#         if lx + lw > rx and ly < ry + rh and ly + lh > ry:  # Check for intersection\n",
    "#             lw = rx - lx  # Adjust left contour width to stop at right contour\n",
    "\n",
    "#     cv2.rectangle(image, (lx, ly), (lx + lw, ly + lh), (0, 0, 255), 2)  # Red color\n",
    "\n",
    "# # Display the image with modified rectangles\n",
    "# cv2.imwrite(\"/content/temp_gridded_c2.png\",image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gfZM9OgaXhJm",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# cnts = cv2.findContours(thresh2, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# cnts = cnts[0] if len(cnts) == 2 else cnts[1]\n",
    "# cnts = sorted(cnts, key=lambda x: cv2.boundingRect(x)[0])\n",
    "# color = (0, 255, 0)  # Green color\n",
    "\n",
    "# for index,c in enumerate(cnts):\n",
    "#   x,y,w,h = cv2.boundingRect(c)\n",
    "#   if index != 0:\n",
    "\n",
    "#     # image[y:y+h+int(h/average_height), x:x+w+int(w/average_width)] = color\n",
    "\n",
    "#     image[y:y+h+10, x:x+w+20] = color\n",
    "# cv2.imwrite(\"/content/temp_rect2.png\",image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vwPXosHPjmqx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the image\n",
    "# image = cv2.imread('/content/temp_rect2.png')\n",
    "\n",
    "# # Convert the image to grayscale\n",
    "# gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # Threshold to detect green regions\n",
    "# lower_green = np.array([0, 150, 0])\n",
    "# upper_green = np.array([0, 255, 0])\n",
    "# mask_green = cv2.inRange(image, lower_green, upper_green)\n",
    "\n",
    "# # Find contours in the green regions\n",
    "# contours, _ = cv2.findContours(mask_green, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Separate contours into left and right columns\n",
    "# left_contours = []\n",
    "# right_contours = []\n",
    "\n",
    "# image_center = image.shape[1] // 2\n",
    "\n",
    "# for cnt in contours:\n",
    "#     x, y, w, h = cv2.boundingRect(cnt)\n",
    "#     if x < image_center:\n",
    "#         left_contours.append((x, y, w, h))\n",
    "#     else:\n",
    "#         right_contours.append((x, y, w, h))\n",
    "\n",
    "# # Draw right column contours first\n",
    "# for (x, y, w, h) in right_contours:\n",
    "#     cv2.rectangle(image, (x, y), (x + w, y + h), (0, 0, 255), 2)  # Red color\n",
    "\n",
    "# # Draw left column contours and adjust them if they intersect with right column boxes\n",
    "# for (lx, ly, lw, lh) in left_contours:\n",
    "#     for (rx, ry, rw, rh) in right_contours:\n",
    "#         if lx + lw > rx and ly < ry + rh and ly + lh > ry:  # Check for intersection\n",
    "#             lw = rx - lx  # Adjust left contour width to stop at right contour\n",
    "\n",
    "#     cv2.rectangle(image, (lx, ly), (lx + lw, ly + lh), (0, 0, 255), 2)  # Red color\n",
    "\n",
    "# # Display the image with modified rectangles\n",
    "# cv2.imwrite(\"/content/temp_gridded.png\",image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZder3GDi9DN",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "\n",
    "# # Load the original image (before green overlay)\n",
    "# original_image = cv2.imread(\"/content/0-0_jpg.rf.91a1b5ecfa392c10b1986ad8070ffb2b11111.jpg\")\n",
    "\n",
    "# # Load the modified image (with green overlay and red boxes)\n",
    "# image_with_green = cv2.imread('/content/temp_gridded.png')\n",
    "\n",
    "# # Convert the image to grayscale\n",
    "# gray = cv2.cvtColor(image_with_green, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# # Threshold to detect green regions\n",
    "# lower_green = np.array([0, 150, 0])\n",
    "# upper_green = np.array([0, 255, 0])\n",
    "# mask_green = cv2.inRange(image_with_green, lower_green, upper_green)\n",
    "\n",
    "# # Find contours in the green regions\n",
    "# contours, _ = cv2.findContours(mask_green, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "# # Separate contours into left and right columns\n",
    "# left_contours = []\n",
    "# right_contours = []\n",
    "\n",
    "# image_center = image_with_green.shape[1] // 2\n",
    "\n",
    "# for cnt in contours:\n",
    "#     x, y, w, h = cv2.boundingRect(cnt)\n",
    "#     if x < image_center:\n",
    "#         left_contours.append((x, y, w, h))\n",
    "#     else:\n",
    "#         right_contours.append((x, y, w, h))\n",
    "\n",
    "# # Draw right column contours first and remove green color by restoring the original image\n",
    "# for (x, y, w, h) in right_contours:\n",
    "#     cv2.rectangle(image_with_green, (x, y), (x + w, y + h), (0, 0, 255), 2)  # Red color\n",
    "#     # Restore the original image in the green regions\n",
    "#     image_with_green[y:y+h, x:x+w][mask_green[y:y+h, x:x+w] == 255] = original_image[y:y+h, x:x+w][mask_green[y:y+h, x:x+w] == 255]\n",
    "\n",
    "# # Draw left column contours, adjust them, and remove green color by restoring the original image\n",
    "# for (lx, ly, lw, lh) in left_contours:\n",
    "#     for (rx, ry, rw, rh) in right_contours:\n",
    "#         if lx + lw > rx and ly < ry + rh and ly + lh > ry:  # Check for intersection\n",
    "#             lw = rx - lx  # Adjust left contour width to stop at right contour\n",
    "\n",
    "#     cv2.rectangle(image_with_green, (lx, ly), (lx + lw, ly + lh), (0, 0, 255), 2)  # Red color\n",
    "#     # Restore the original image in the green regions\n",
    "#     image_with_green[ly:ly+lh, lx:lx+lw][mask_green[ly:ly+lh, lx:lx+lw] == 255] = original_image[ly:ly+lh, lx:lx+lw][mask_green[ly:ly+lh, lx:lx+lw] == 255]\n",
    "\n",
    "# # Display the image with the green regions removed and text visible\n",
    "# # cv2.imshow(\"Restored Image\", image_with_green)\n",
    "# cv2.imwrite(\"/content/temp_gridded.png\",image_with_green)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 474,
     "status": "ok",
     "timestamp": 1723880132185,
     "user": {
      "displayName": "Rudra Banerjee",
      "userId": "04281573669841337427"
     },
     "user_tz": -330
    },
    "id": "LYrYnIG0jp7e",
    "outputId": "11738bd3-e338-42f1-feb4-343adb4aa72a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# temp_gridded_c1 = cv2.imread(\"/content/temp_gridded_c1.png\")\n",
    "# temp_gridded_c2 = cv2.imread(\"/content/temp_gridded_c2.png\")\n",
    "# original_image = cv2.imread(\"/content/0-0_jpg.rf.91a1b5ecfa392c10b1986ad8070ffb2b11111.jpg\")\n",
    "# combined_gridded = cv2.bitwise_or(temp_gridded_c1, temp_gridded_c2)\n",
    "# cv2.imwrite(\"/content/combined_gridded.png\",combined_gridded)\n",
    "# cv2.imwrite(\"/content/combined_gridded.png\",image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYPK4HfEeBVr"
   },
   "source": [
    "# **Text take out**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5737031,
     "sourceId": 9440782,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5740624,
     "sourceId": 9445604,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6258180,
     "sourceId": 10139819,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6295525,
     "sourceId": 10189737,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30761,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
